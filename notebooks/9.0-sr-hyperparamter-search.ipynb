{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/suriya/dcu/Group-Project\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.rssi_distance import extract_feature2, postproc_feature_dicts\n",
    "from src.featutils import aggregate_features_from_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"data/tc4tl_training_data_v1/tc4tl/data/train/\"\n",
    "train_key = pd.read_csv(\"data/tc4tl_training_data_v1/tc4tl/docs/tc4tl_train_key.tsv\", sep=\"\\t\")\n",
    "test_dir = \"data/tc4tl_data_v5/tc4tl/data/test/\"\n",
    "test_mdata = pd.read_csv(\"data/tc4tl_data_v5/tc4tl/docs/tc4tl_test_metadata.tsv\", sep=\"\\t\")\n",
    "test_key = pd.read_csv(\"data/tc4tl_test_key/tc4tl/docs/tc4tl_test_key.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, _ = aggregate_features_from_folder(train_dir, train_key, feat_fn=extract_feature2, postproc_fn=postproc_feature_dicts)\n",
    "testset, _  = aggregate_features_from_folder(test_dir, test_key, feat_fn=extract_feature2, postproc_fn=postproc_feature_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(trainset['PredictedDistance'].values.reshape(-1, 1), trainset['Distance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rf.predict(testset['PredictedDistance'].values.reshape(-1, 1))\n",
    "(out == testset['Distance']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(max_features=0.01)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.CoarseGrain = trainset.CoarseGrain.replace({\n",
    "    'Y' : 0., 'N' : 1.\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(trainset[['PredictedDistance', 'CoarseGrain']], trainset['Distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.CoarseGrain = testset.CoarseGrain.replace({\n",
    "    'Y' : 0., 'N' : 1.\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = best_estimator.predict(testset[['PredictedDistance', 'CoarseGrain']])\n",
    "(out == testset['Distance']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Hyperparameter Search leads to {(3664 - 2853)/2853}% increase in accuracy!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tune-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tune_sklearn import TuneGridSearchCV\n",
    "\n",
    "_param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    # 'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "tune_search = TuneGridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    _param_grid,\n",
    "    early_stopping=True,\n",
    "    max_iters=10\n",
    ")\n",
    "grid_search.fit(trainset[['PredictedDistance', 'CoarseGrain']], trainset['Distance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tune-sklearn` is just marginally faster than `GridSearchCV`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperopt-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hpsklearn import HyperoptEstimator\n",
    "\n",
    "# Load Data\n",
    "train_features, train_labels = trainset[['PredictedDistance', 'CoarseGrain']], trainset['Distance']\n",
    "test_features, test_labels = testset[['PredictedDistance', 'CoarseGrain']], testset['Distance']\n",
    "\n",
    "# Create the estimator object\n",
    "estim = HyperoptEstimator()\n",
    "\n",
    "# Search the space of classifiers and preprocessing steps and their\n",
    "# respective hyperparameters in sklearn to fit a model to the data\n",
    "estim.fit(train_features, train_labels)\n",
    "\n",
    "# Report the accuracy of the classifier on a given set of data\n",
    "score = estim.score(test_features, test_labels)\n",
    "\n",
    "# Return instances of the classifier and preprocessing steps\n",
    "model = estim.best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(estim.predict(test_features) == test_labels).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3770 / len(test_features), 3664/len(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Tunables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network.multilayer_perceptron import MLPClassifier\n",
    "from sklearn.neighbors.classification import RadiusNeighborsClassifier\n",
    "from sklearn.neighbors.classification import KNeighborsClassifier\n",
    "from sklearn.linear_model.stochastic_gradient import SGDClassifier\n",
    "from sklearn.linear_model.ridge import RidgeClassifier\n",
    "# from sklearn.gaussian_process.gpc import GaussianProcessClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "_classifiers = [\n",
    "    ExtraTreeClassifier,\n",
    "    DecisionTreeClassifier,\n",
    "    MLPClassifier,\n",
    "    KNeighborsClassifier,\n",
    "    SGDClassifier,\n",
    "    RidgeClassifier,\n",
    "    # GaussianProcessClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    BaggingClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    RandomForestClassifier,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def evaluate(features, labels, classifiers=[], verbose=False):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.33, random_state=69)\n",
    "    scores = []\n",
    "    if len(classifiers) == 0:\n",
    "        classifiers = _classifiers\n",
    "    iterate_through = tqdm(classifiers) if verbose else classifiers\n",
    "    for _classifier in iterate_through:\n",
    "        model = _classifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        scores.append((model.predict(X_test) == y_test).mean())\n",
    "        if verbose:\n",
    "            iterate_through.set_description(f\"{_classifier.__name__}: {scores[-1]}\")\n",
    "    return max(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def objective(params):\n",
    "    # TX, N = params[0][-1], params[1][-1]\n",
    "    TX, N = params['TX'], params['N']\n",
    "    trainset, _ = aggregate_features_from_folder(\n",
    "        train_dir, train_key, feat_fn=extract_feature2, postproc_fn=postproc_feature_dicts,\n",
    "        tunables={'TX' : TX, 'N' : N}, testing=True, verbose=False\n",
    "    )\n",
    "    trainset.CoarseGrain = trainset.CoarseGrain.replace({\n",
    "        'Y' : 0., 'N' : 1.\n",
    "    })\n",
    "    train_features, train_labels = trainset[['PredictedDistance', 'CoarseGrain']], trainset['Distance']\n",
    "    return -evaluate(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:11<00:26,  1.91s/trial, best loss: -0.6272727272727273]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=4.87145e-25): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:21<00:17,  1.97s/trial, best loss: -0.6272727272727273]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=2.08134e-55): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:39<00:00,  1.96s/trial, best loss: -0.6272727272727273]\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp, fmin, tpe\n",
    "\n",
    "space = {\n",
    "    'TX' : hp.uniform('TX', -80, -40),\n",
    "    'N' : hp.uniform('N', 0.1, 5)\n",
    "}\n",
    "best = fmin(fn=objective, algo=tpe.suggest,\n",
    "           space=space, max_evals=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'N': 4.115621637416503, 'TX': -43.23022252718384}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15552/15552 [00:12<00:00, 1198.79it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d49f9c266b9496f954a7209e24cd260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5690629261640366"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset, _ = aggregate_features_from_folder(\n",
    "        train_dir, train_key, feat_fn=extract_feature2, postproc_fn=postproc_feature_dicts,\n",
    "        tunables=best, testing=False, verbose=True\n",
    "    )\n",
    "trainset.CoarseGrain = trainset.CoarseGrain.replace({\n",
    "        'Y' : 0., 'N' : 1.\n",
    "    })\n",
    "train_features, train_labels = trainset[['PredictedDistance', 'CoarseGrain']], trainset['Distance']\n",
    "evaluate(train_features, train_labels, verbose=True, classifiers=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## src.hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hyperopt import optimize_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:30<00:10,  2.07s/trial, best loss: -0.5969696969696969]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=4.77665e-29): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:40<00:00,  2.03s/trial, best loss: -0.5969696969696969]\n"
     ]
    }
   ],
   "source": [
    "def make_data(tunables):\n",
    "    trainset, _ = aggregate_features_from_folder(\n",
    "                train_dir, train_key, feat_fn=extract_feature2, postproc_fn=postproc_feature_dicts,\n",
    "                tunables=tunables, testing=True, verbose=False\n",
    "            )\n",
    "    trainset.CoarseGrain = trainset.CoarseGrain.replace({\n",
    "        'Y' : 0., 'N' : 1.\n",
    "    })\n",
    "    train_features, train_labels = trainset[['PredictedDistance', 'CoarseGrain']], trainset['Distance']\n",
    "    return train_features, train_labels\n",
    "\n",
    "space = {\n",
    "    'TX' : hp.uniform('TX', -80, -40),\n",
    "    'N' : hp.uniform('N', 0.1, 5)\n",
    "}\n",
    "\n",
    "best_params = optimize_preproc(make_data, space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hyperopt import optimize_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.13s/trial, best loss: 0.45499999999999996]\n",
      "[10:09:55] WARNING: ../src/learner.cc:516:           \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "100%|██████████| 2/2 [00:02<00:00,  2.24s/trial, best loss: 0.43500000000000005]\n",
      "100%|██████████| 3/3 [00:02<00:00,  2.89s/trial, best loss: 0.43500000000000005]\n",
      "100%|██████████| 4/4 [00:00<00:00,  2.25trial/s, best loss: 0.43500000000000005]\n",
      "100%|██████████| 5/5 [00:00<00:00,  6.37trial/s, best loss: 0.43500000000000005]\n",
      "100%|██████████| 6/6 [00:00<00:00,  4.56trial/s, best loss: 0.43500000000000005]\n",
      "100%|██████████| 7/7 [00:13<00:00, 13.07s/trial, best loss: 0.43500000000000005]\n",
      "[10:10:15] WARNING: ../src/learner.cc:516:           \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "100%|██████████| 8/8 [00:02<00:00,  2.91s/trial, best loss: 0.43500000000000005]\n",
      "100%|██████████| 9/9 [00:00<00:00, 29.28trial/s, best loss: 0.43500000000000005]\n",
      "100%|██████████| 10/10 [00:06<00:00,  6.15s/trial, best loss: 0.43500000000000005]\n",
      "[10:10:24] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = make_data(best_params)\n",
    "estim, best_model = optimize_hp(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "              colsample_bylevel=0.8427257640994569, colsample_bynode=1,\n",
       "              colsample_bytree=0.6526318542603939, gamma=0.1546020413234132,\n",
       "              gpu_id=-1, importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.005093704505070369, max_delta_step=0, max_depth=1,\n",
       "              min_child_weight=22, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=6000, n_jobs=1, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=1,\n",
       "              reg_alpha=0.594809519556231, reg_lambda=3.5750379568729524,\n",
       "              scale_pos_weight=1, seed=1, subsample=0.9958139856405576,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model['learner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 642.65it/s]\n"
     ]
    }
   ],
   "source": [
    "testset, _ = aggregate_features_from_folder(\n",
    "            test_dir, test_key, feat_fn=extract_feature2, postproc_fn=postproc_feature_dicts,\n",
    "            tunables=best_params, testing=True, verbose=True\n",
    "        )\n",
    "testset.CoarseGrain = testset.CoarseGrain.replace({\n",
    "    'Y' : 0., 'N' : 1.\n",
    "})\n",
    "test_features, test_labels = trainset[['PredictedDistance', 'CoarseGrain']], trainset['Distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33359053497942387"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estim.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estim.score(train_features, train_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
